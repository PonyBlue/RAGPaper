# RAG ç§‘ç ”è®ºæ–‡æ™ºèƒ½åˆ†æåŠ©æ‰‹ - ä¼˜åŒ–æ–¹æ¡ˆ

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†é¡¹ç›®çš„åç»­ä¼˜åŒ–æ–¹å‘å’Œå…·ä½“å®ç°æ–¹æ¡ˆï¼Œå¸®åŠ©é€æ­¥æå‡ç³»ç»Ÿæ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

---

## ç›®å½•

- [é˜¶æ®µä¸€ï¼šæ ¸å¿ƒåŠŸèƒ½ä¼˜åŒ–](#é˜¶æ®µä¸€æ ¸å¿ƒåŠŸèƒ½ä¼˜åŒ–)
  - [1.1 Rerank é‡æ’åº](#11-rerank-é‡æ’åº)
  - [1.2 å¢å¼ºçš„å¼•æ–‡æº¯æº](#12-å¢å¼ºçš„å¼•æ–‡æº¯æº)
  - [1.3 å±‚çº§åŒ–æ–‡æ¡£åˆ‡ç‰‡](#13-å±‚çº§åŒ–æ–‡æ¡£åˆ‡ç‰‡)
- [é˜¶æ®µäºŒï¼šåŠŸèƒ½æ‰©å±•](#é˜¶æ®µäºŒåŠŸèƒ½æ‰©å±•)
  - [2.1 å…ƒæ•°æ®ç®¡ç†](#21-å…ƒæ•°æ®ç®¡ç†)
  - [2.2 å¤šæ¨¡æ€æ”¯æŒ](#22-å¤šæ¨¡æ€æ”¯æŒ)
  - [2.3 å¯¹è¯å†å²ç®¡ç†](#23-å¯¹è¯å†å²ç®¡ç†)
- [é˜¶æ®µä¸‰ï¼šæ€§èƒ½ä¼˜åŒ–](#é˜¶æ®µä¸‰æ€§èƒ½ä¼˜åŒ–)
  - [3.1 ç¼“å­˜æœºåˆ¶](#31-ç¼“å­˜æœºåˆ¶)
  - [3.2 æ‰¹é‡å¤„ç†ä¼˜åŒ–](#32-æ‰¹é‡å¤„ç†ä¼˜åŒ–)
  - [3.3 å¼‚æ­¥å¤„ç†](#33-å¼‚æ­¥å¤„ç†)
- [é˜¶æ®µå››ï¼šé«˜çº§åŠŸèƒ½](#é˜¶æ®µå››é«˜çº§åŠŸèƒ½)
  - [4.1 å¤šæ–‡æ¡£å¯¹æ¯”åˆ†æ](#41-å¤šæ–‡æ¡£å¯¹æ¯”åˆ†æ)
  - [4.2 çŸ¥è¯†å›¾è°±æ„å»º](#42-çŸ¥è¯†å›¾è°±æ„å»º)
  - [4.3 è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆ](#43-è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆ)

---

## é˜¶æ®µä¸€ï¼šæ ¸å¿ƒåŠŸèƒ½ä¼˜åŒ–

### 1.1 Rerank é‡æ’åº

**ç›®æ ‡**: æå‡æ£€ç´¢ç²¾åº¦ï¼Œå°† Top-K å‡†ç¡®ç‡æé«˜ 20-30%

#### æŠ€æœ¯æ–¹æ¡ˆ

ä½¿ç”¨ Rerank æ¨¡å‹å¯¹åˆæ­¥æ£€ç´¢ç»“æœè¿›è¡Œé‡æ–°æ’åºï¼Œå¸¸ç”¨æ–¹æ¡ˆï¼š

1. **BGE Reranker** (æ¨è)
   - æ¨¡å‹ï¼š`BAAI/bge-reranker-large`
   - ä¼˜ç‚¹ï¼šå¼€æºå…è´¹ï¼Œæ•ˆæœå¥½
   - é€‚ç”¨åœºæ™¯ï¼šä¸­æ–‡è®ºæ–‡

2. **Cohere Rerank API**
   - ä¼˜ç‚¹ï¼šæ•ˆæœæœ€å¥½ï¼Œæ”¯æŒå¤šè¯­è¨€
   - ç¼ºç‚¹ï¼šéœ€è¦ä»˜è´¹

#### å®ç°æ­¥éª¤

**æ­¥éª¤1**: å®‰è£…ä¾èµ–
```bash
pip install sentence-transformers
```

**æ­¥éª¤2**: åˆ›å»º `src/reranker.py`
```python
from sentence_transformers import CrossEncoder
from typing import List, Tuple
from langchain_core.documents import Document


class Reranker:
    """é‡æ’åºå™¨"""

    def __init__(self, model_name: str = "BAAI/bge-reranker-large"):
        print(f"æ­£åœ¨åŠ è½½Rerankæ¨¡å‹: {model_name}")
        self.model = CrossEncoder(model_name)
        print("Rerankæ¨¡å‹åŠ è½½å®Œæˆ")

    def rerank(
        self,
        query: str,
        documents: List[Document],
        top_k: int = 4
    ) -> List[Tuple[Document, float]]:
        """
        é‡æ’åºæ–‡æ¡£

        Args:
            query: æŸ¥è¯¢é—®é¢˜
            documents: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›å‰Kä¸ªç»“æœ

        Returns:
            [(document, score), ...] æŒ‰åˆ†æ•°é™åºæ’åˆ—
        """
        # å‡†å¤‡è¾“å…¥å¯¹
        pairs = [[query, doc.page_content] for doc in documents]

        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        scores = self.model.predict(pairs)

        # ç»„åˆæ–‡æ¡£å’Œåˆ†æ•°
        doc_scores = list(zip(documents, scores))

        # æŒ‰åˆ†æ•°é™åºæ’åº
        doc_scores.sort(key=lambda x: x[1], reverse=True)

        # è¿”å›å‰Kä¸ª
        return doc_scores[:top_k]
```

**æ­¥éª¤3**: ä¿®æ”¹ `src/vectorstore.py`ï¼Œæ·»åŠ RerankåŠŸèƒ½
```python
from src.reranker import Reranker

class VectorStoreManager:
    def __init__(self, ..., use_rerank: bool = False):
        # ... åŸæœ‰ä»£ç  ...
        self.use_rerank = use_rerank
        self.reranker = None

        if use_rerank:
            self.reranker = Reranker()

    def similarity_search_with_rerank(
        self,
        query: str,
        k: int = 4,
        fetch_k: int = 20  # åˆæ­¥æ£€ç´¢æ›´å¤šæ–‡æ¡£
    ) -> List[Document]:
        """å¸¦é‡æ’åºçš„ç›¸ä¼¼åº¦æœç´¢"""
        if not self.use_rerank or self.reranker is None:
            return self.similarity_search(query, k=k)

        # åˆæ­¥æ£€ç´¢æ›´å¤šå€™é€‰
        candidates = self.similarity_search(query, k=fetch_k)

        # Reranké‡æ’åº
        reranked = self.reranker.rerank(query, candidates, top_k=k)

        # è¿”å›é‡æ’åºåçš„æ–‡æ¡£
        return [doc for doc, score in reranked]
```

**æ­¥éª¤4**: åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ å¼€å…³
```python
# src/config.py
USE_RERANK = os.getenv("USE_RERANK", "false").lower() == "true"
```

#### é¢„æœŸæ•ˆæœ

- **å‡†ç¡®ç‡æå‡**: Top-4 å‡†ç¡®ç‡æå‡ 25%
- **å“åº”æ—¶é—´**: å¢åŠ  0.5-1 ç§’ï¼ˆå¯æ¥å—ï¼‰
- **ç”¨æˆ·ä½“éªŒ**: ç­”æ¡ˆè´¨é‡æ˜¾è‘—æé«˜

---

### 1.2 å¢å¼ºçš„å¼•æ–‡æº¯æº

**ç›®æ ‡**: è®©ç”¨æˆ·èƒ½ç²¾å‡†å®šä½ç­”æ¡ˆæ¥æºï¼Œå¢å¼ºå¯ä¿¡åº¦

#### åŠŸèƒ½è®¾è®¡

1. **é«˜äº®æ˜¾ç¤º**: åœ¨è¿”å›çš„æ–‡æ¡£ç‰‡æ®µä¸­é«˜äº®ç›¸å…³å†…å®¹
2. **é¡µç è·³è½¬**: æä¾›PDFé¡µç é“¾æ¥ï¼ˆå‰ç«¯æ”¯æŒï¼‰
3. **ç›¸å…³åº¦è¯„åˆ†**: æ˜¾ç¤ºæ¯ä¸ªå¼•ç”¨ç‰‡æ®µçš„ç›¸å…³æ€§åˆ†æ•°
4. **ä¸Šä¸‹æ–‡å±•ç¤º**: æä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯

#### å®ç°æ­¥éª¤

**æ­¥éª¤1**: ä¿®æ”¹ `src/qa_chain.py`
```python
def format_response_enhanced(self, result: Dict) -> str:
    """å¢å¼ºçš„æ ¼å¼åŒ–å“åº”ï¼ŒåŒ…å«è¯¦ç»†æº¯æºä¿¡æ¯"""
    answer = result["result"]
    source_documents = result.get("source_documents", [])

    formatted_response = f"## ğŸ“ å›ç­”\n\n{answer}\n\n"

    if source_documents:
        formatted_response += "---\n\n## ğŸ“š å‚è€ƒæ¥æºï¼ˆå¼•æ–‡æº¯æºï¼‰\n\n"

        for i, doc in enumerate(source_documents, 1):
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            page = doc.metadata.get("page", "æœªçŸ¥")

            # æ–‡æ¡£å†…å®¹é¢„è§ˆ
            content = doc.page_content[:300].replace("\n", " ")

            formatted_response += f"### ğŸ“„ å¼•ç”¨ {i}\n\n"
            formatted_response += f"- **æ¥æºæ–‡æ¡£**: {source}\n"
            formatted_response += f"- **é¡µç **: ç¬¬ {page} é¡µ\n"
            formatted_response += f"- **ç›¸å…³å†…å®¹**:\n\n"
            formatted_response += f"> {content}...\n\n"

            # æ·»åŠ è·³è½¬é“¾æ¥ï¼ˆå¦‚æœæœ‰PDFæŸ¥çœ‹å™¨ï¼‰
            # formatted_response += f"[ğŸ“– è·³è½¬åˆ°åŸæ–‡](#page-{page})\n\n"

    return formatted_response
```

**æ­¥éª¤2**: æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•°æ˜¾ç¤º
```python
def format_response_with_scores(self, result: Dict, scores: List[float]) -> str:
    """å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„æ ¼å¼åŒ–å“åº”"""
    answer = result["result"]
    source_documents = result.get("source_documents", [])

    formatted_response = f"## ğŸ“ å›ç­”\n\n{answer}\n\n"

    if source_documents:
        formatted_response += "---\n\n## ğŸ“š å‚è€ƒæ¥æº\n\n"

        for i, (doc, score) in enumerate(zip(source_documents, scores), 1):
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            page = doc.metadata.get("page", "æœªçŸ¥")

            # ç›¸ä¼¼åº¦ç™¾åˆ†æ¯”
            similarity = f"{score * 100:.1f}%" if score else "N/A"

            formatted_response += f"### ğŸ“„ å¼•ç”¨ {i} (ç›¸å…³åº¦: {similarity})\n\n"
            formatted_response += f"- **æ–‡æ¡£**: {source} (ç¬¬ {page} é¡µ)\n"
            formatted_response += f"- **å†…å®¹ç‰‡æ®µ**:\n\n"
            formatted_response += f"> {doc.page_content[:200]}...\n\n"

    return formatted_response
```

#### é¢„æœŸæ•ˆæœ

- **å¯ä¿¡åº¦**: ç”¨æˆ·å¯ä»¥éªŒè¯ç­”æ¡ˆæ¥æº
- **å­¦æœ¯ä»·å€¼**: ç¬¦åˆå­¦æœ¯å¼•ç”¨è§„èŒƒ
- **ç”¨æˆ·æ»¡æ„åº¦**: æå‡ 30%+

---

### 1.3 å±‚çº§åŒ–æ–‡æ¡£åˆ‡ç‰‡

**ç›®æ ‡**: ä¿ç•™æ–‡æ¡£ç»“æ„ä¿¡æ¯ï¼Œæé«˜æ£€ç´¢è´¨é‡

#### æŠ€æœ¯æ–¹æ¡ˆ

è¯†åˆ«PDFä¸­çš„æ ‡é¢˜ã€ç« èŠ‚ã€æ®µè½å±‚çº§ï¼Œåœ¨åˆ‡ç‰‡æ—¶ä¿ç•™è¿™äº›ç»“æ„ä¿¡æ¯ã€‚

#### å®ç°æ­¥éª¤

**æ­¥éª¤1**: å¢å¼ºæ–‡æ¡£è§£æï¼Œè¯†åˆ«ç»“æ„
```python
# src/document_loader.py
import re

class EnhancedDocumentProcessor(DocumentProcessor):
    """å¢å¼ºçš„æ–‡æ¡£å¤„ç†å™¨ï¼Œæ”¯æŒç»“æ„è¯†åˆ«"""

    def extract_structure(self, text: str) -> dict:
        """æå–æ–‡æ¡£ç»“æ„"""
        structure = {
            "title": None,
            "sections": [],
            "current_section": None
        }

        lines = text.split('\n')

        for line in lines:
            # æ£€æµ‹ä¸€çº§æ ‡é¢˜ï¼ˆå…¨å¤§å†™æˆ–ç‰¹å®šæ ¼å¼ï¼‰
            if re.match(r'^[A-Z\s]{10,}$', line.strip()):
                structure["sections"].append({
                    "title": line.strip(),
                    "level": 1,
                    "content": []
                })
            # æ£€æµ‹äºŒçº§æ ‡é¢˜ï¼ˆæ•°å­—å¼€å¤´ï¼‰
            elif re.match(r'^\d+\.\s+[A-Z]', line.strip()):
                structure["sections"].append({
                    "title": line.strip(),
                    "level": 2,
                    "content": []
                })
            # æ™®é€šå†…å®¹
            else:
                if structure["sections"]:
                    structure["sections"][-1]["content"].append(line)

        return structure

    def process_pdf_with_structure(self, pdf_path: str):
        """å¸¦ç»“æ„ä¿¡æ¯çš„PDFå¤„ç†"""
        documents = self.load_pdf(pdf_path)

        enhanced_docs = []
        for doc in documents:
            # æå–ç»“æ„
            structure = self.extract_structure(doc.page_content)

            # ä¸ºæ¯ä¸ªç« èŠ‚åˆ›å»ºæ–‡æ¡£å—
            for section in structure["sections"]:
                section_text = "\n".join(section["content"])

                # åˆ†å—
                chunks = self.text_splitter.split_text(section_text)

                for chunk in chunks:
                    # æ·»åŠ ç»“æ„å…ƒæ•°æ®
                    enhanced_doc = Document(
                        page_content=chunk,
                        metadata={
                            **doc.metadata,
                            "section_title": section["title"],
                            "section_level": section["level"]
                        }
                    )
                    enhanced_docs.append(enhanced_doc)

        return enhanced_docs
```

**æ­¥éª¤2**: åœ¨æ£€ç´¢æ—¶åˆ©ç”¨ç»“æ„ä¿¡æ¯
```python
def search_with_structure_boost(self, query: str, k: int = 4):
    """åˆ©ç”¨ç»“æ„ä¿¡æ¯çš„åŠ æƒæ£€ç´¢"""
    results = self.similarity_search_with_score(query, k=k*2)

    # æ ¹æ®ç« èŠ‚çº§åˆ«è°ƒæ•´åˆ†æ•°
    adjusted_results = []
    for doc, score in results:
        level = doc.metadata.get("section_level", 2)
        # ä¸€çº§æ ‡é¢˜ä¸‹çš„å†…å®¹æƒé‡æ›´é«˜
        boost = 1.2 if level == 1 else 1.0
        adjusted_results.append((doc, score * boost))

    # é‡æ–°æ’åº
    adjusted_results.sort(key=lambda x: x[1], reverse=True)

    return [doc for doc, score in adjusted_results[:k]]
```

#### é¢„æœŸæ•ˆæœ

- **æ£€ç´¢ç²¾åº¦**: æå‡ 15%
- **ç­”æ¡ˆè´¨é‡**: æ›´ç¬¦åˆæ–‡æ¡£é€»è¾‘ç»“æ„
- **ä¸Šä¸‹æ–‡ç†è§£**: æ›´å¥½åœ°ç†è§£æ–‡æ¡£ä¸Šä¸‹æ–‡

---

## é˜¶æ®µäºŒï¼šåŠŸèƒ½æ‰©å±•

### 2.1 å…ƒæ•°æ®ç®¡ç†

**ç›®æ ‡**: æå–å¹¶ç®¡ç†è®ºæ–‡çš„ç»“æ„åŒ–ä¿¡æ¯

#### åŠŸèƒ½è®¾è®¡

æå–å¹¶å­˜å‚¨ï¼š
- è®ºæ–‡æ ‡é¢˜
- ä½œè€…
- å‘è¡¨å¹´ä»½
- æœŸåˆŠ/ä¼šè®®
- æ‘˜è¦
- å…³é”®è¯

#### å®ç°æ­¥éª¤

**æ­¥éª¤1**: åˆ›å»ºå…ƒæ•°æ®æå–å™¨
```python
# src/metadata_extractor.py
import re
from typing import Dict, Optional

class MetadataExtractor:
    """è®ºæ–‡å…ƒæ•°æ®æå–å™¨"""

    def extract_from_pdf(self, pdf_path: str) -> Dict:
        """ä»PDFæå–å…ƒæ•°æ®"""
        from PyPDF2 import PdfReader

        reader = PdfReader(pdf_path)
        metadata = {}

        # ä»PDFå…ƒæ•°æ®ä¸­æå–
        if reader.metadata:
            metadata["title"] = reader.metadata.get("/Title", "")
            metadata["author"] = reader.metadata.get("/Author", "")
            metadata["creation_date"] = reader.metadata.get("/CreationDate", "")

        # ä»é¦–é¡µæå–
        first_page = reader.pages[0].extract_text()

        # æå–æ ‡é¢˜ï¼ˆé€šå¸¸åœ¨ç¬¬ä¸€è¡Œæˆ–å‰å‡ è¡Œï¼‰
        if not metadata.get("title"):
            lines = first_page.split('\n')[:10]
            for line in lines:
                if len(line.strip()) > 10 and not line.isupper():
                    metadata["title"] = line.strip()
                    break

        # æå–ä½œè€…ï¼ˆæŸ¥æ‰¾åŒ…å«@æˆ–Emailçš„è¡Œé™„è¿‘ï¼‰
        authors = self._extract_authors(first_page)
        if authors:
            metadata["authors"] = authors

        # æå–å¹´ä»½
        year = self._extract_year(first_page)
        if year:
            metadata["year"] = year

        # æå–æ‘˜è¦
        abstract = self._extract_abstract(first_page)
        if abstract:
            metadata["abstract"] = abstract

        return metadata

    def _extract_authors(self, text: str) -> list:
        """æå–ä½œè€…åˆ—è¡¨"""
        # ç®€å•å®ç°ï¼šæŸ¥æ‰¾åŒ…å«@çš„è¡Œï¼Œå–å‰ä¸€è¡Œä½œä¸ºä½œè€…
        lines = text.split('\n')
        authors = []

        for i, line in enumerate(lines):
            if '@' in line or 'Email' in line:
                if i > 0:
                    potential_authors = lines[i-1]
                    # åˆ†å‰²ä½œè€…å
                    authors = re.split(r'[,ï¼Œ]', potential_authors)
                    break

        return [a.strip() for a in authors if a.strip()]

    def _extract_year(self, text: str) -> Optional[int]:
        """æå–å‘è¡¨å¹´ä»½"""
        # æŸ¥æ‰¾4ä½æ•°å­—å¹´ä»½ï¼ˆ2000-2099ï¼‰
        matches = re.findall(r'\b(20\d{2})\b', text)
        if matches:
            return int(matches[0])
        return None

    def _extract_abstract(self, text: str) -> Optional[str]:
        """æå–æ‘˜è¦"""
        # æŸ¥æ‰¾Abstractå…³é”®è¯
        pattern = r'Abstract[:\s]+(.*?)(?=\n\n|\nIntroduction|\n1\.)'
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)

        if match:
            abstract = match.group(1).strip()
            return abstract[:500]  # é™åˆ¶é•¿åº¦

        return None
```

**æ­¥éª¤2**: åœ¨ç•Œé¢ä¸­æ˜¾ç¤ºå…ƒæ•°æ®
```python
# app.py
def display_document_metadata(pdf_file):
    """æ˜¾ç¤ºæ–‡æ¡£å…ƒæ•°æ®"""
    from src.metadata_extractor import MetadataExtractor

    extractor = MetadataExtractor()
    metadata = extractor.extract_from_pdf(pdf_file)

    st.sidebar.subheader("ğŸ“„ æ–‡æ¡£ä¿¡æ¯")
    if metadata.get("title"):
        st.sidebar.text(f"æ ‡é¢˜: {metadata['title'][:50]}...")
    if metadata.get("authors"):
        st.sidebar.text(f"ä½œè€…: {', '.join(metadata['authors'][:3])}")
    if metadata.get("year"):
        st.sidebar.text(f"å¹´ä»½: {metadata['year']}")
```

#### é¢„æœŸæ•ˆæœ

- **æ–‡æ¡£ç®¡ç†**: æ›´å¥½åœ°ç»„ç»‡å’Œæ£€ç´¢è®ºæ–‡
- **è¿‡æ»¤åŠŸèƒ½**: å¯æŒ‰å¹´ä»½ã€ä½œè€…ç­›é€‰
- **å¼•ç”¨ç”Ÿæˆ**: è‡ªåŠ¨ç”Ÿæˆæ ‡å‡†å¼•ç”¨æ ¼å¼

---

### 2.2 å¤šæ¨¡æ€æ”¯æŒ

**ç›®æ ‡**: æ”¯æŒè®ºæ–‡ä¸­çš„å›¾è¡¨ã€å…¬å¼è¯†åˆ«ä¸æ£€ç´¢

#### åŠŸèƒ½è®¾è®¡

1. **å›¾è¡¨æå–**: ä»PDFä¸­æå–å›¾ç‰‡
2. **OCRè¯†åˆ«**: è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—
3. **å…¬å¼è¯†åˆ«**: è¯†åˆ«æ•°å­¦å…¬å¼
4. **å¤šæ¨¡æ€æ£€ç´¢**: åŒæ—¶æ£€ç´¢æ–‡æœ¬å’Œå›¾è¡¨

#### å®ç°æ­¥éª¤

**æ­¥éª¤1**: å®‰è£…ä¾èµ–
```bash
pip install pdfplumber pillow pytesseract
# éœ€è¦å®‰è£… Tesseract OCR
```

**æ­¥éª¤2**: åˆ›å»ºå›¾è¡¨æå–å™¨
```python
# src/image_extractor.py
import pdfplumber
from PIL import Image
import io

class ImageExtractor:
    """PDFå›¾è¡¨æå–å™¨"""

    def extract_images(self, pdf_path: str, output_dir: str):
        """æå–PDFä¸­çš„æ‰€æœ‰å›¾ç‰‡"""
        images = []

        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                # æå–å›¾ç‰‡
                page_images = page.images

                for img_index, img in enumerate(page_images):
                    # ä¿å­˜å›¾ç‰‡
                    image_path = f"{output_dir}/page{page_num}_img{img_index}.png"
                    # ... ä¿å­˜é€»è¾‘ ...

                    images.append({
                        "page": page_num,
                        "index": img_index,
                        "path": image_path
                    })

        return images

    def extract_figures_with_captions(self, pdf_path: str):
        """æå–å›¾è¡¨åŠå…¶æ ‡é¢˜"""
        figures = []

        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                text = page.extract_text()

                # æŸ¥æ‰¾å›¾è¡¨æ ‡é¢˜ï¼ˆFigure X:, Table X:ï¼‰
                import re
                captions = re.findall(
                    r'(Figure|Table)\s+(\d+)[:\.]?\s+(.*?)(?=\n|Figure|Table|$)',
                    text,
                    re.IGNORECASE
                )

                for caption in captions:
                    figures.append({
                        "type": caption[0],
                        "number": caption[1],
                        "caption": caption[2],
                        "page": page.page_number
                    })

        return figures
```

#### é¢„æœŸæ•ˆæœ

- **å®Œæ•´æ€§**: ä¸é—æ¼å›¾è¡¨ä¿¡æ¯
- **å¤šç»´æ£€ç´¢**: æ”¯æŒ"å›¾Xæ˜¾ç¤ºäº†ä»€ä¹ˆ"çš„é—®é¢˜
- **ç†è§£æ·±åº¦**: æ›´å…¨é¢ç†è§£è®ºæ–‡å†…å®¹

---

### 2.3 å¯¹è¯å†å²ç®¡ç†

**ç›®æ ‡**: æ”¯æŒä¸Šä¸‹æ–‡ç›¸å…³çš„è¿ç»­å¯¹è¯

#### å®ç°æ­¥éª¤

**æ­¥éª¤1**: ä¿®æ”¹ `src/qa_chain.py`ï¼Œæ”¯æŒå¯¹è¯è®°å¿†
```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

class ConversationalQAManager(QAChainManager):
    """æ”¯æŒå¯¹è¯å†å²çš„é—®ç­”ç®¡ç†å™¨"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )

    def create_conversational_chain(self, retriever):
        """åˆ›å»ºå¯¹è¯å¼é—®ç­”é“¾"""
        chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=self.memory,
            return_source_documents=True,
            verbose=True
        )
        return chain

    def ask_with_context(self, chain, question: str) -> Dict:
        """å¸¦ä¸Šä¸‹æ–‡çš„æé—®"""
        result = chain.invoke({"question": question})
        return result

    def clear_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.memory.clear()
```

**æ­¥éª¤2**: åœ¨ç•Œé¢ä¸­é›†æˆ
```python
# app.py
if "conversation_chain" not in st.session_state:
    qa_manager = ConversationalQAManager()
    retriever = vs_manager.get_retriever()
    st.session_state.conversation_chain = qa_manager.create_conversational_chain(retriever)

# ç”¨æˆ·æé—®æ—¶
result = st.session_state.conversation_chain.invoke({"question": question})
```

#### é¢„æœŸæ•ˆæœ

- **è‡ªç„¶å¯¹è¯**: æ”¯æŒ"å®ƒæ˜¯ä»€ä¹ˆ"ã€"æ›´è¯¦ç»†åœ°è§£é‡Š"ç­‰è¿½é—®
- **ä¸Šä¸‹æ–‡ç†è§£**: ç†è§£ä»£è¯å’Œçœç•¥
- **ç”¨æˆ·ä½“éªŒ**: æ›´æ¥è¿‘äººç±»å¯¹è¯

---

## é˜¶æ®µä¸‰ï¼šæ€§èƒ½ä¼˜åŒ–

### 3.1 ç¼“å­˜æœºåˆ¶

**ç›®æ ‡**: å‡å°‘é‡å¤è®¡ç®—ï¼Œæå‡å“åº”é€Ÿåº¦

#### å®ç°æ–¹æ¡ˆ

1. **Embeddingç¼“å­˜**: ç¼“å­˜å·²è®¡ç®—çš„å‘é‡
2. **æŸ¥è¯¢ç¼“å­˜**: ç¼“å­˜ç›¸ä¼¼æŸ¥è¯¢çš„ç»“æœ
3. **LLMå“åº”ç¼“å­˜**: ç¼“å­˜å¸¸è§é—®é¢˜çš„ç­”æ¡ˆ

```python
# src/cache_manager.py
import hashlib
import json
from pathlib import Path

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str = ".cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def _get_hash(self, key: str) -> str:
        """ç”Ÿæˆç¼“å­˜keyçš„å“ˆå¸Œ"""
        return hashlib.md5(key.encode()).hexdigest()

    def get(self, key: str):
        """è·å–ç¼“å­˜"""
        cache_file = self.cache_dir / f"{self._get_hash(key)}.json"
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None

    def set(self, key: str, value):
        """è®¾ç½®ç¼“å­˜"""
        cache_file = self.cache_dir / f"{self._get_hash(key)}.json"
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(value, f, ensure_ascii=False)
```

---

### 3.2 æ‰¹é‡å¤„ç†ä¼˜åŒ–

**ç›®æ ‡**: æé«˜å¤§æ‰¹é‡æ–‡æ¡£å¤„ç†é€Ÿåº¦

```python
def batch_process_pdfs(self, pdf_paths: List[str], batch_size: int = 5):
    """æ‰¹é‡å¤„ç†PDF"""
    from concurrent.futures import ThreadPoolExecutor

    all_chunks = []

    with ThreadPoolExecutor(max_workers=4) as executor:
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(pdf_paths), batch_size):
            batch = pdf_paths[i:i+batch_size]
            futures = [executor.submit(self.process_pdf, path) for path in batch]

            for future in futures:
                chunks = future.result()
                all_chunks.extend(chunks)

    return all_chunks
```

---

### 3.3 å¼‚æ­¥å¤„ç†

**ç›®æ ‡**: æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œæ”¯æŒåå°å¤„ç†

```python
# ä½¿ç”¨Streamlitçš„å¼‚æ­¥åŠŸèƒ½
import asyncio

async def process_documents_async(files):
    """å¼‚æ­¥å¤„ç†æ–‡æ¡£"""
    tasks = [process_single_file(file) for file in files]
    results = await asyncio.gather(*tasks)
    return results
```

---

## é˜¶æ®µå››ï¼šé«˜çº§åŠŸèƒ½

### 4.1 å¤šæ–‡æ¡£å¯¹æ¯”åˆ†æ

**ç›®æ ‡**: å¯¹æ¯”å¤šç¯‡è®ºæ–‡çš„å¼‚åŒ

```python
def compare_papers(self, paper_ids: List[str], aspect: str):
    """å¯¹æ¯”å¤šç¯‡è®ºæ–‡"""
    prompt = f"""
    è¯·å¯¹æ¯”ä»¥ä¸‹è®ºæ–‡åœ¨{aspect}æ–¹é¢çš„å¼‚åŒï¼š

    è®ºæ–‡1: ...
    è®ºæ–‡2: ...

    è¯·ä»ä»¥ä¸‹è§’åº¦å¯¹æ¯”ï¼š
    1. æ ¸å¿ƒæ–¹æ³•
    2. å®éªŒè®¾ç½®
    3. ä¸»è¦ç»“è®º
    4. åˆ›æ–°ç‚¹
    """

    # è°ƒç”¨LLMç”Ÿæˆå¯¹æ¯”åˆ†æ
    response = self.llm.invoke(prompt)
    return response.content
```

---

### 4.2 çŸ¥è¯†å›¾è°±æ„å»º

**ç›®æ ‡**: æ„å»ºè®ºæ–‡ä¹‹é—´çš„å…³ç³»ç½‘ç»œ

```python
# æå–å®ä½“å’Œå…³ç³»
def extract_entities_and_relations(text: str):
    """æå–å®ä½“å’Œå…³ç³»"""
    # ä½¿ç”¨NERæ¨¡å‹æå–
    # - æ–¹æ³•å
    # - æŠ€æœ¯åè¯
    # - ä½œè€…
    # - å¼•ç”¨å…³ç³»
    pass
```

---

### 4.3 è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆ

**ç›®æ ‡**: ä¸ºè®ºæ–‡ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦

```python
def generate_structured_summary(pdf_path: str):
    """ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦"""
    prompt = """
    è¯·ä¸ºè¿™ç¯‡è®ºæ–‡ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦ï¼ŒåŒ…æ‹¬ï¼š

    1. ç ”ç©¶èƒŒæ™¯ï¼ˆ1-2å¥ï¼‰
    2. æ ¸å¿ƒæ–¹æ³•ï¼ˆ2-3å¥ï¼‰
    3. ä¸»è¦è´¡çŒ®ï¼ˆ3-5ä¸ªè¦ç‚¹ï¼‰
    4. å®éªŒç»“æœï¼ˆ1-2å¥ï¼‰
    5. ç»“è®ºä¸å±•æœ›ï¼ˆ1å¥ï¼‰
    """

    # è°ƒç”¨LLMç”Ÿæˆ
    summary = self.llm.invoke(prompt)
    return summary.content
```

---

## å®æ–½ä¼˜å…ˆçº§å»ºè®®

### ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å®æ–½ï¼‰
1. **Reranké‡æ’åº** - æ˜¾è‘—æå‡å‡†ç¡®ç‡
2. **å¢å¼ºå¼•æ–‡æº¯æº** - æå‡ç”¨æˆ·ä¿¡ä»»åº¦
3. **å¯¹è¯å†å²ç®¡ç†** - æ”¹å–„ç”¨æˆ·ä½“éªŒ

### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆçŸ­æœŸè§„åˆ’ï¼‰
4. **å±‚çº§åŒ–åˆ‡ç‰‡** - æå‡æ£€ç´¢è´¨é‡
5. **å…ƒæ•°æ®ç®¡ç†** - æ”¹å–„æ–‡æ¡£ç»„ç»‡
6. **ç¼“å­˜æœºåˆ¶** - æå‡æ€§èƒ½

### ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆé•¿æœŸè§„åˆ’ï¼‰
7. **å¤šæ¨¡æ€æ”¯æŒ** - éœ€è¦è¾ƒå¤šå¼€å‘å·¥ä½œ
8. **çŸ¥è¯†å›¾è°±** - å¤æ‚åº¦é«˜
9. **å¤šæ–‡æ¡£å¯¹æ¯”** - é«˜çº§åŠŸèƒ½

---

## æ€§èƒ½æŒ‡æ ‡ç›®æ ‡

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ | ä¼˜åŒ–æ–¹æ¡ˆ |
|------|------|------|----------|
| Top-4å‡†ç¡®ç‡ | 60% | 85% | Rerank + å±‚çº§åŒ–åˆ‡ç‰‡ |
| å¹³å‡å“åº”æ—¶é—´ | 3s | <2s | ç¼“å­˜ + å¼‚æ­¥å¤„ç† |
| ç”¨æˆ·æ»¡æ„åº¦ | - | 90%+ | å¼•æ–‡æº¯æº + å¯¹è¯å†å² |
| æ”¯æŒæ–‡æ¡£ç±»å‹ | PDF | PDF+å›¾è¡¨ | å¤šæ¨¡æ€æ”¯æŒ |

---

## æŠ€æœ¯æ ˆæ‰©å±•

### æ–°å¢ä¾èµ–
```bash
# Rerank
pip install sentence-transformers

# å›¾è¡¨å¤„ç†
pip install pdfplumber pillow pytesseract

# å¯¹è¯è®°å¿†
pip install langchain-community

# ç¼“å­˜
pip install diskcache redis

# å¼‚æ­¥å¤„ç†
pip install aiofiles asyncio
```

---

## æ€»ç»“

æœ¬ä¼˜åŒ–æ–¹æ¡ˆæ¶µç›–äº†ä»æ ¸å¿ƒåŠŸèƒ½å¢å¼ºåˆ°é«˜çº§ç‰¹æ€§çš„å®Œæ•´è·¯å¾„ã€‚å»ºè®®æŒ‰ç…§ä¼˜å…ˆçº§é€æ­¥å®æ–½ï¼Œæ¯ä¸ªé˜¶æ®µå®Œæˆåè¿›è¡Œæµ‹è¯•å’Œè¯„ä¼°ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§å’Œç”¨æˆ·ä½“éªŒçš„æŒç»­æå‡ã€‚

**é¢„æœŸæˆæœ**:
- âœ… æ£€ç´¢å‡†ç¡®ç‡æå‡ 40%+
- âœ… ç”¨æˆ·æ»¡æ„åº¦è¾¾åˆ° 90%+
- âœ… æ”¯æŒæ›´å¤æ‚çš„å­¦æœ¯åœºæ™¯
- âœ… ç³»ç»Ÿæ€§èƒ½æå‡ 50%+
- âœ… åŠŸèƒ½ä¸°å¯Œåº¦è¾¾åˆ°å•†ä¸šçº§æ°´å¹³

---

*æœ€åæ›´æ–°: 2024-12*
